# Fuentes útiles

Aquí encontrarás una lista de fuentes de **ejemplos practicos de la integración de Openai con MCP**.

* [mcp-openai](https://github.com/S1M0N38/mcp-openai):
  * Este es un cliente MCP (no un servidor). Está pensado para ser usado como una librería para construir LLMs UI que soporten MCP a través de una API compatible con OpenAI. Esto abre la puerta a motores de inferencia ejecutables localmente (vLLM, Ollama, TGI, llama.cpp, LMStudio, ...) que soportan proporcionar soporte para la API OpenAI (generación de texto, llamada a funciones, etc.).
* **[openai-mcp-client](https://github.com/ResoluteError/openai-mcp-client)**:
  * Este es un ejemplo simple de cómo usar el Protocolo de Contexto de Modelo (MCP) con la API de OpenAI para crear un agente simple actuando desde un contexto de chat.

**Ejecutar modelos LLM en local**:

1. LMStudio: [https://lmstudio.ai/](https://lmstudio.ai/) --> De momento parece una solución muy potente para el desarrollo
2. Ollama

Adaptadores de MCP para  Langchain:

* [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters)
